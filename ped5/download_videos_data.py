# TODO regoinCode: The parameter value is an ISO 3166-1 alpha-2 country code. (string), GB and US
# TODO publishDate: as in original dataset
# TODO trendingDate: as in original dataset
# TODO maybe chart: mostPopular
# TODO using api key
# Base attrs:
# video_id - /search;
# trending_date - skipped -> generated by data set author;
# title - /search;
# channel_title - /videos by id;
# category_id - /videos by id;
# publish_time - publishedAt /search;
# tags - /videos by id;
# views - /videos by id;
# likes - /videos by id;
# dislikes - /videos by id;
# comment_count - /videos by id;
# thumbnail_link - /search thumbnails->default->url;
# comments_disabled - /videos by id;
# ratings_disabled  - /videos by id;
# video_error_or_removed - /videos by id;
# description - /search
import json
import os
import traceback
from argparse import ArgumentParser
from typing import List

import pandas as pd
import requests

# Date format: 2017-12-25T20:21:57.000Z
# Min publishedAfter: 2017-11-01T00:00:00.000Z
# Max publishedAfter: 2018-06-30T00:00:00.000Z
from helpers.categories import get_categories_dict
from helpers.files import load_csv


def setup_args_parser() -> ArgumentParser:
    args_parser = ArgumentParser()
    args_parser.add_argument("--published_after", help="Format: yyyy-MM-ddTHH:mm:ssZ", type=str,
                             default="2017-11-01T00:00:00Z")
    args_parser.add_argument("--published_before", help="Format: yyyy-MM-ddTHH:mm:ssZ", type=str,
                             default="2018-06-30T00:00:00Z")
    return args_parser


def get_category_ids():
    gb_data, us_data = load_csv("clustering_data")

    videos = pd.concat([gb_data, us_data])
    # inconsistency in data
    videos["category_id"] = videos["category_id"].replace(43.0, 24.0)
    category_ids = videos["new_category_id"].dropna().unique().tolist()
    print(category_ids)
    names_dict = get_categories_dict()
    names = [names_dict[i] for i in category_ids]
    return names


def download_video_details(video_id: str):
    # TODO implement
    pass


def get_trending_ids(region_code: str) -> List[str]:
    if region_code == "GB":
        data, _ = load_csv("youtube_grouped_by_id")
    elif region_code == "US":
        _, data = load_csv("youtube_grouped_by_id")
    else:
        raise ValueError(f"No country {region_code}")
    return data["new_video_id"].tolist()


# TODO implement it
def map_items_to_data(items, excluded_ids):
    print(items)
    mapped_items = []
    for item in items:
        kind = item["id"]["kind"]
        if kind != "youtube#video":
            continue
        videoId = item["id"]["videoId"]
        if videoId in excluded_ids:
            continue
        else:
            mapped_items.append(item)
    print(f"MappedLen: {len(mapped_items)}")
    return mapped_items


def main(args):
    category_ids = get_category_ids()
    gb_data = download_data("GB", category_ids, args)
    # us_data = download_data("US")


def download_data(region_code: str, category_ids: list, args: dict) -> pd.DataFrame:
    excluded_ids = get_trending_ids(region_code)
    api_key = os.getenv("API_KEY")
    # TODO iterate over categories
    total_count = 0
    next_page_token = ""
    print(args)
    # f"publishedAfter={args['published_after']}&" \
    # f"publishedBefore={args['published_before']}&" \
    #  TODO add dates
    while next_page_token is not None and total_count < 100:
        try:
            link = f"https://www.googleapis.com/youtube/v3/search?" \
                   f"chart=mostPopular&" \
                   f"regionCode={region_code}&" \
                   f"{next_page_token}" \
                   f"maxResults=50&" \
                   f"part=snippet&" \
                   f"key={api_key}"
            print(link)

            response = requests.get(link)
            print(response)
            body = response.content.decode("utf-8")
            body = json.loads(body)
            if response.status_code == 200:
                print(body)
                data = map_items_to_data(body['items'], excluded_ids)
                next_page_token = f"pageToken={body['nextPageToken']}&"
                # TODO increment total count
                results = body['pageInfo']['resultsPerPage']  # TODO filter from trending
                total_count += min(len(data), results)
                print(f"Total count: {total_count}")
            else:
                print(body)
                next_page_token = None

        except Exception as e:
            traceback.print_exc()
            print(f"Error: {e}")
            exit(-156)


if __name__ == '__main__':
    parser = setup_args_parser()
    main(vars(parser.parse_args()))
