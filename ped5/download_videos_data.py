# TODO regoinCode: The parameter value is an ISO 3166-1 alpha-2 country code. (string), GB and US
# TODO publishDate: as in original dataset
# TODO trendingDate: as in original dataset
# TODO maybe chart: mostPopular
# TODO using api key
# Base attrs:

# video_id - /search;
# trending_date - skipped -> generated by data set author;
# title - /search;
# description - /search
# thumbnail_link - /search thumbnails->default->url;
# publish_time - publishedAt /search;

# channel_title - /videos by id;
# category_id - /videos by id;
# tags - /videos by id;
# views - /videos by id;
# likes - /videos by id;
# dislikes - /videos by id;
# comment_count - /videos by id;
# comments_disabled - /videos by id;
# ratings_disabled  - /videos by id;
# video_error_or_removed - /videos by id;

import json
import os
import sys
import traceback
from argparse import ArgumentParser
from typing import List, Optional, Tuple

import pandas as pd
import requests
# Date format: 2017-12-25T20:21:57.000Z
# Min publishedAfter: 2017-11-01T00:00:00.000Z
# Max publishedAfter: 2018-06-30T00:00:00.000Z
from tqdm import tqdm

from helpers.files import load_csv, save_csv


# curl \
#   'https://youtube.googleapis.com/youtube/v3/search?maxResults=50&publishedAfter=2017-11-01T00%3A00%3A00Z&publishedBefore=2018-06-30T00%3A00%3A00Z&regionCode=GB&type=video&videoCategoryId=2&key=[YOUR_API_KEY]' \
#   --header 'Accept: application/json' \
#   --compressed


def setup_args_parser() -> ArgumentParser:
    args_parser = ArgumentParser()
    args_parser.add_argument("--published_after", help="Format: yyyy-MM-ddTHH:mm:ssZ", type=str,
                             default="2017-11-01T00:00:00Z")
    args_parser.add_argument("--published_before", help="Format: yyyy-MM-ddTHH:mm:ssZ", type=str,
                             default="2018-06-30T00:00:00Z")
    return args_parser


def get_category_ids():
    gb_data, us_data = load_csv("clustering_data")

    videos = pd.concat([gb_data, us_data])
    # inconsistency in data
    videos["category_id"] = videos["category_id"].replace(43.0, 24.0)
    category_ids = videos["new_category_id"].dropna().unique().tolist()
    print(category_ids)
    # names_dict = get_categories_dict()
    # names = [names_dict[i] for i in category_ids]
    # return names
    return [str(int(x)) for x in category_ids]


def download_video_details(video_id: str, region_code: str) -> Optional[dict]:
    api_key = os.getenv("API_KEY")
    link = f"https://www.googleapis.com/youtube/v3/videos?id={video_id}&part=snippet%2Cstatistics&key={api_key}"
    try:
        response = requests.get(link)
        if response.status_code != 200:
            return None
        body = json.loads(response.content.decode("utf-8"))
        if len(body["items"]) < 1:
            return None

        item_body = body["items"][0]
        snippet = item_body["snippet"]
        statistics = item_body["statistics"]

        title = snippet.get("title", "")
        thumbnail = snippet.get("thumbnails", dict()).get("default", dict()).get("url", "")
        description = snippet.get("description", "")
        channel_title = snippet.get("channelTitle", "")
        category_id = snippet.get("categoryId", "")
        published_time = snippet.get("publishedAt", "")
        tags = "|".join(snippet.get("tags", []))
        views = statistics.get("viewCount", 0)

        if 'likeCount' in statistics and 'dislikeCount' in statistics:
            likes = statistics['likeCount']
            dislikes = statistics['dislikeCount']
            ratings_disabled = False
        else:
            ratings_disabled = True
            likes = 0
            dislikes = 0

        if 'commentCount' in statistics:
            comment_count = statistics['commentCount']
            comments_disabled = False
        else:
            comments_disabled = True
            comment_count = 0
        return {
            "video_id": video_id,
            "title": title,
            "channel_title": channel_title,
            "category_id": category_id,
            "publish_time": published_time,
            "tags": tags,
            "views": views,
            "likes": likes,
            "dislikes": dislikes,
            "comment_count": comment_count,
            "thumbnail_link": thumbnail,
            "comments_disabled": comments_disabled,
            "ratings_disabled": ratings_disabled,
            "video_error_or_removed": False,
            "description": description
        }
    except Exception as e:
        traceback.print_exc()
        print(f"Error by getting video ({video_id}): {e}")
        return None


def get_trending_ids(region_code: str) -> List[str]:
    if region_code == "GB":
        data, _ = load_csv("youtube_grouped_by_id")
    elif region_code == "US":
        _, data = load_csv("youtube_grouped_by_id")
    else:
        raise ValueError(f"No country {region_code}")
    return data["new_video_id"].tolist()


def map_items_to_data(items, category_ids, excluded_ids, region_code, downloaded_ids) -> Tuple[pd.DataFrame, List[str]]:
    mapped_items = {
        "video_id": [],
        "title": [],
        "channel_title": [],
        "category_id": [],
        "publish_time": [],
        "tags": [],
        "views": [],
        "likes": [],
        "dislikes": [],
        "comment_count": [],
        "thumbnail_link": [],
        "comments_disabled": [],
        "ratings_disabled": [],
        "video_error_or_removed": [],
        "description": []
    }
    not_video = 0
    excluded = 0
    not_category = 0
    none_count = 0
    downloaded_count = 0
    for item in tqdm(items):
        kind = item["id"]["kind"]
        if kind != "youtube#video":
            not_video += 1
            continue
        video_id = item.get("id", dict()).get("videoId", "")
        if video_id in excluded_ids or video_id == "":
            excluded += 1
            continue
        if video_id in downloaded_ids:
            downloaded_count += 1
            continue
        else:
            item_data = download_video_details(video_id, region_code)
            if item_data is None:
                none_count += 1
                continue
            if item_data["category_id"] not in category_ids:
                not_category += 1
                continue
            if item_data is not None:
                downloaded_ids.append(video_id)
                for key in item_data.keys():
                    mapped_items[key].append(item_data[key])
    print(f"Excluded ids count: {excluded}")
    print(f"Excluded categories count: {not_category}")
    print(f"Not video count: {not_video}")
    print(f"None count: {none_count}")
    print(f"Already downloaded: {downloaded_count}")
    data = pd.DataFrame(data=mapped_items)
    print(f"MappedLen: {len(data)}")
    return data, downloaded_ids


def main(args):
    category_ids = get_category_ids()
    gb_data = download_data("GB", category_ids, args)
    # us_data = download_data("US")


def download_data(region_code: str, category_ids: list, args: dict) -> pd.DataFrame:
    path = os.path.dirname(__file__)
    path = os.path.join(path, "..", "ped5_data")
    print(path)
    os.makedirs(path, exist_ok=True)
    excluded_ids = get_trending_ids(region_code)
    api_key = os.getenv("API_KEY")
    total_count = 0
    next_page_token = ""
    print(args)
    videos = pd.DataFrame()
    downloaded_ids = []
    # TODO iterate over categories
    # TODO time into 3 or 6 intervals
    category_id = 10
    # while next_page_token is not None and total_count < 100:
    #     try:
    #         link = f"https://www.googleapis.com/youtube/v3/search?" \
    #                f"chart=mostPopular&" \
    #                f"regionCode={region_code}&" \
    #                f"{next_page_token}" \
    #                f"type=video&" \
    #                f"videoCategoryId={category_id}" \
    #                f"publishedAfter={args['published_after']}&" \
    #                f"publishedBefore={args['published_before']}&" \
    #                f"maxResults=50&" \
    #                f"part=snippet&" \
    #                f"key={api_key}"
    #         print(link)
    #
    #         response = requests.get(link)
    #         print(response)
    #         body = response.content.decode("utf-8")
    #         body = json.loads(body)
    #         if response.status_code == 200:
    #             print(body)
    #             page_info = body.get("pageInfo", dict())
    #             print(f"TotalResults: {page_info.get('totalResults', None)}")
    #             items = body.get("items", [])
    #             data, downloaded_ids = map_items_to_data(items, category_ids, excluded_ids, region_code, downloaded_ids)
    #             videos = pd.concat([videos, data])
    #             page_token = body.get("nextPageToken", None)
    #             if page_token is None:
    #                 next_page_token = None
    #             else:
    #                 next_page_token = f"pageToken={body['nextPageToken']}&"
    #             results = body['pageInfo']['resultsPerPage']  # TODO filter from trending
    #             total_count += min(len(data), results)
    #             print(f"Total count: {total_count}")
    #         else:
    #             print(body, file=sys.stderr)
    #             next_page_token = None
    #
    #     except Exception as e:
    #         traceback.print_exc()
    #         print(f"Error: {e}")
    #         exit(-156)
    save_csv("ped5_data", [videos], [f"{region_code}_videos"])


if __name__ == '__main__':
    parser = setup_args_parser()
    main(vars(parser.parse_args()))
